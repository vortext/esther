* Progress
** DONE hook up OpenAI
** DONE store results
- sqllite storing memories
** DONE use results, better context (locale, time, memories)
** DONE markdown, KaTeX and such
** DONE /inspect command
** DONE /status command
** DONE styling according to style vibes
** DONE user flow (just use sqllite)
** DONE vibes for login page
** DONE logout functionality (/logout)
** DONE refactor memory and login/auth controllers
** DONE geoip for time of day?
** DONE maybe bouncing ball energy for loading?
** DONE introduce as book fiction literary work imaginary scene
** DONE keyword frecency query
** DONE multi user?
_ encrypt contents
_ encrypt keywords
_ add vault to user
_ uid in vault
_ more user information in session (vault)
** DONE use data, iv as columns instead of the json for memory and memory_keywords
** DONE better input UI? (textfield, resize, like Signal?)
** DONE local version of fonts
** DONE just slurp the secrets.edn file and get rid of the rest (secrets.clj)
** DONE refactor memory to only have contents
** DONE error handling
** DONE better integrate keywords in narrative
** DONE check multi-line markdown
** DONE input checking and validation (also converse.clj)
** DONE Remember and show for a day, clear page every day (aligns with the prominent date)
** DONE /clear command (clear all memory for uid)
** DONE url handling (it renders)
** DONE check logging levels (disable debug outside dev)
** DONE llma locally
** DONE llama tweaks
- json grammar :D
- performance tweaks (gpu layers, cores)
** DONE keep for llama.cpp
- calculate token-length (and set)
- up the context
** DONE try openorca-platypus2-13b
- Requires different prompt template format see https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B
** DONE get rid of sid
** DONE experiment with multi-turn
** DONE weather API integration
** DONE fix mutli-line input scroll to bottom of page (css?)
** DONE time of day etc context in prompt
** DONE history on first request
** DONE deprecate openai
** DONE harmonize markdown handling using graalvm
** DONE get rid of pty_bridy.py
** DONE switch to guff
- probably needs lots of updating
** DONE Try https://huggingface.co/Gryphe/MythoMax-L2-13b
- MythoMax-L2-13B-GGML-Q6_K with llamacpp, and Mirostat set to Mirostat 2, Tau 5, Eta 0.1
- https://www.reddit.com/r/LocalLLaMA/comments/167088h/why_are_the_answers_getting_dumber_as_the/
- different prompt format
** TODO use proper locale for times?
** TODO better handling of location data
** TODO clear cache of llama.cpp daily (integrate quartz)
** TODO use a single map from the whole req-rep thread with namespaced keywords.
** TODO link parsing?
** TODO ui things
- Discoverability of input bar
- Button for archive or other commands?
** TODO integrate https://github.com/spencermountain/compromise?
** TODO embeddings for memory retrieval and context
- Add memories during converse into associative mem
- Retrieve similar memories (if any) => guids (associate)
- Inject similar memories in user request context
- https://github.com/nmslib/hnswlib (via JNA clong clang?)
- split out api gen so don't depend on the whole of llvm
- does it need the stdlib in the api?
- get it to work simple example
- get it to work persistence, how to deal with scoping to user?!
- get it to work with embeddings and real data (find or generate some dataset)
- how to generate the proper amount of context? (summarization???)
** TODO embeddings
- onnx bert?
- bert.cpp / jna?
- wait for llama.cpp to fix it?
** TODO harmonize emoji handling using graalvm?
** TODO keyword_memory_lookup table and logic
** TODO dark mode
** TODO better intro flow
** TODO /help command
** TODO sign-up page
** TODO weird bug where login shows in memory.
** TODO better error handling subprocesses
** TODO calendar page (for memories)?
** TODO share a page (static gen)
** TODO security and hosting
** TODO test infrastructure and tests
** TODO deploy and build shizzle (Github actions?)
** TODO app? legal, ethics, billing, subscriptions, etc.
* Ideas for later
** TODO image upload (img-to-text?)
** TODO offline desktop app?
** TODO /imagine command (no solutions for this ATM)
- hook up image generation (StableDiffusionAPI is too crappy)
- integrate image describe into memory
- https://github.com/deep-floyd/IF maybe?
- store description and image in memory contents
- use image describe as context for converse
- stablediffusion locally?
