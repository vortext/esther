* Progress
** DONE hook up OpenAI
** DONE store results
- sqllite storing memories
** DONE use results, better context (locale, time, memories)
** DONE markdown, KaTeX and such
** DONE /inspect command
** DONE /status command
** DONE styling according to style vibes
** DONE user flow (just use sqllite)
** DONE vibes for login page
** DONE logout functionality (/logout)
** DONE refactor memory and login/auth controllers
** DONE geoip for time of day?
** DONE maybe bouncing ball energy for loading?
** DONE introduce as book fiction literary work imaginary scene
** DONE keyword frecency query
** DONE multi user?
_ encrypt contents
_ encrypt keywords
_ add vault to user
_ uid in vault
_ more user information in session (vault)
** DONE use data, iv as columns instead of the json for memory and memory_keywords
** DONE better input UI? (textfield, resize, like Signal?)
** DONE local version of fonts
** DONE just slurp the secrets.edn file and get rid of the rest (secrets.clj)
** DONE refactor memory to only have contents
** DONE error handling
** DONE better integrate keywords in narrative
** DONE check multi-line markdown
** DONE input checking and validation (also converse.clj)
** DONE Remember and show for a day, clear page every day (aligns with the prominent date)
** DONE /clear command (clear all memory for uid)
** DONE url handling (it renders)
** DONE check logging levels (disable debug outside dev)
** DONE llma locally
** DONE llama tweaks
- json grammar :D
- performance tweaks (gpu layers, cores)
** DONE keep for llama.cpp
- calculate token-length (and set)
- up the context
** DONE try openorca-platypus2-13b
- Requires different prompt template format see https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B
** DONE get rid of sid
** DONE experiment with multi-turn
** DONE weather API integration
** DONE fix mutli-line input scroll to bottom of page (css?)
** DONE time of day etc context in prompt
** DONE history on first request
** DONE deprecate openai
** DONE harmonize markdown handling using graalvm
** DONE get rid of pty_bridy.py
** DONE switch to guff
- probably needs lots of updating
** DONE Try https://huggingface.co/Gryphe/MythoMax-L2-13b
- MythoMax-L2-13B-GGML-Q6_K with llamacpp, and Mirostat set to Mirostat 2, Tau 5, Eta 0.1
- https://www.reddit.com/r/LocalLLaMA/comments/167088h/why_are_the_answers_getting_dumber_as_the/
- different prompt format
** DONE use proper locale for times?
** DONE better handling of location data
** DONE move all the local context stuff to server side (e.g. moon phase etc) -> create proper context.clj controller
** DONE use ðŸŒ’ emojis in prompt again (they're cute)
** DONE figure out a more sane TZ to heuristic lat long
** DONE proper timezone for human-today on initial render (see also https://ipgeolocation.io/timezone-api.html)
** DONE set guards around weather in prompt-template
** DONE use a single map from the whole req-rep thread with namespaced keywords.
** DONE minify on the fly with hash check (maybe wontfix because javascript is hell)
** DONE Gzip
** DONE harmonize emoji handling using graalvm? (did it a better way client side)
** DONE keyword_memory_lookup table and logic
- can done with jvm ring-middleware
- could speed up with clong or llvm for practice?
- but really belongs in nginx or wherever
** TODO move llama parameters into system.edn
** TODO disable colors in logging when prod
** TODO logo + favicon
** TODO daily scheduled tasks: sleep, dream, clean (integrate quartz)
** TODO start llama when converse UI is loaded (refactor the whole impl thing)?
** TODO set context rollover properly (e.g. from day to night) when subproc is running?
** TODO export / download data functionality (for me and GDPR!)
** TODO figure out a sane way to do multi user scalably
** TODO check if there's a distributed deploy strategy that supports mirostat ðŸ™ƒ
** TODO ui things
- Discoverability of input bar
- Button for archive or other commands?
** TODO test infrastructure and tests => generate training / test data for fine tuning
** TODO embeddings for memory retrieval and context
- Add memories during converse into associative mem
- Retrieve similar memories (if any) => guids (associate)
- For prompt: frecency keywords => memories by keyword => most_similar => summarize top-k => context: {kw: summary}
- For converse: most_similar => summarize top-k => context: {kw: summary}
- Inject similar memories in user request context
- https://github.com/nmslib/hnswlib (via JNA clong clang?)
- get it to work simple example
- get it to work persistence, how to deal with scoping to user?!
- get it to work with embeddings and real data (find or generate some dataset)
- how to generate the proper amount of context? (summarization???)
** TODO embeddings
- onnx bert?
- bert.cpp / jna?
- wait for llama.cpp to fix it?
** TODO dark mode
** TODO a way to serialize "personality"
** TODO better intro flow
** TODO /help command
** TODO sign-up page
** TODO integrate https://github.com/spencermountain/compromise for NER or for the lulz?
** TODO weird bug where login shows in memory (enforce redirect).
** DONE better error handling subprocesses
** TODO calendar page (for memories)?
** TODO semantic search memories
** TODO share a page (static gen)
** TODO security and hosting
** TODO deploy and build shizzle (Github actions?)
** TODO app? legal, ethics, billing, subscriptions, etc.
* Ideas for later
** TODO finetune model
** TODO link parsing in that it downloads the page and adds it as context
** TODO image upload (img-to-text?)
** TODO offline desktop app?
** TODO /imagine command (no solutions for this ATM)
- hook up image generation (StableDiffusionAPI is too crappy)
- integrate image describe into memory
- https://github.com/deep-floyd/IF maybe?
- store description and image in memory contents
- use image describe as context for converse
- stablediffusion locally?
